{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T09:58:28.258081Z",
     "start_time": "2018-05-08T09:58:27.578956Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import os\n",
    "from os.path import exists, join\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "import spacy\n",
    "\n",
    "import copy\n",
    "\n",
    "import ujson as uj\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T09:58:30.363991Z",
     "start_time": "2018-05-08T09:58:30.350825Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_dir = '../Data/Datasets'\n",
    "emb_dir = '../Data/Embeddings'\n",
    "\n",
    "train_filename = 'train-v1.1.json'\n",
    "dev_filename = 'dev-v1.1.json'\n",
    "char_emb_filename = \"glove.840B.300d-char.txt\"\n",
    "word_emb_zip = \"glove.840B.300d.zip\"\n",
    "word_emb_filename = \"glove.840B.300d.txt\"\n",
    "\n",
    "word_emb_url_base = \"http://nlp.stanford.edu/data/\"\n",
    "char_emb_url_base = \"https://raw.githubusercontent.com/minimaxir/char-embeddings/master/\"\n",
    "train_url_base = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/\"\n",
    "dev_url_base = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/\"\n",
    "\n",
    "spec_toks = ['<UNK>', '<PAD>']\n",
    "\n",
    "CONTEXT_MAX_LEN = 300\n",
    "QUESTION_MAX_LEN = 60\n",
    "ASCII_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T09:58:30.936071Z",
     "start_time": "2018-05-08T09:58:30.896165Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_DIM = 128\n",
    "BATCH_SIZE = 32\n",
    "WORD_EMB_SIZE = 300\n",
    "CHAR_EMB_SIZE = 200\n",
    "\n",
    "WORD_MAX_LEN = 16\n",
    "TENSORS_IS_TRAINABLE = True\n",
    "\n",
    "# Dropout\n",
    "DROP_PROB_WD = 0.1\n",
    "DROP_PROB_CH = 0.05\n",
    "DROP_PROB_INNER = 0.1\n",
    "DROP_PROB_ATTEN = 0.1\n",
    "\n",
    "\n",
    "# WordCharEmbedding\n",
    "WORDCHAR_EMBED_KERSIZE = 5\n",
    "N_HIGHWAY_LAYERS = 2\n",
    "\n",
    "\n",
    "# SelfAttention\n",
    "N_HEADS = 8\n",
    "\n",
    "\n",
    "# QANet\n",
    "N_EMBED_ENC_CONVS = 4\n",
    "EMBED_ENC_KERSIZE = 7\n",
    "\n",
    "N_MODEL_ENC_CONVS = 2\n",
    "MODEL_ENC_KERSIZE = 7\n",
    "\n",
    "DOWNSCALE_KERSIZE = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T09:58:33.679348Z",
     "start_time": "2018-05-08T09:58:33.669766Z"
    }
   },
   "outputs": [],
   "source": [
    "class Embeddings:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        word_emb: word embeddings\n",
    "        char_emb: char embeddings\n",
    "        wd2id: word to index\n",
    "        id2wd: index to word\n",
    "        ch2id: char to index\n",
    "        id2ch: index to char\n",
    "        '''\n",
    "        self.word_emb = np.empty(1)\n",
    "        self.char_emb = np.empty(1)\n",
    "        self.wd2id = {}\n",
    "        self.id2wd = {}\n",
    "        self.ch2id = {}\n",
    "        self.id2ch = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T09:58:34.279992Z",
     "start_time": "2018-05-08T09:58:34.272271Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    '''\n",
    "    length: the dataset lenght\n",
    "    context: list of passages\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.length = 0\n",
    "        self.contexts = []\n",
    "        self.questions = []\n",
    "        self.questions_id = []\n",
    "        self.answers = []\n",
    "        self.packs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T09:58:35.098064Z",
     "start_time": "2018-05-08T09:58:35.090957Z"
    }
   },
   "outputs": [],
   "source": [
    "def download(url, path, filename):\n",
    "    '''\n",
    "    Downloads the file from url/filename and saves in path/filename\n",
    "    '''\n",
    "    if (exists(join(path, filename))):\n",
    "        print(\"File {} already exists.\".format(filename))\n",
    "        return\n",
    "    print(\"Downloading file {}...\".format(filename))\n",
    "    urllib.request.urlretrieve(join(url, filename), filename=join(path, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T09:58:35.887556Z",
     "start_time": "2018-05-08T09:58:35.858605Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_all_data():\n",
    "    '''\n",
    "    Downloads embeddings and datasets\n",
    "    '''\n",
    "    dirs = [dataset_dir, emb_dir]\n",
    "    \n",
    "    for d in dirs:\n",
    "        if not exists(d):\n",
    "            os.makedirs(d)\n",
    "    \n",
    "    download(train_url_base, dataset_dir, train_filename)\n",
    "    download(dev_url_base, dataset_dir, dev_filename)\n",
    "    download(char_emb_url_base, emb_dir, char_emb_filename)\n",
    "    \n",
    "    if not os.path.exists(join(emb_dir, word_emb_filename)):\n",
    "        download(word_emb_url_base, emb_dir, word_emb_zip)\n",
    "        print(\"Unzipping file {}\".format(word_emb_zip))\n",
    "        zip_path = join(emb_dir, word_emb_zip)\n",
    "        txt_path = join(emb_dir, word_emb_filename)\n",
    "        zp = zipfile.ZipFile(zip_path)\n",
    "        zp.extractall(path=txt_path)\n",
    "        zp.close()\n",
    "        os.remove(zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T09:58:37.377551Z",
     "start_time": "2018-05-08T09:58:37.273345Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_glove_embeddings(embed: Embeddings):\n",
    "    '''\n",
    "    Parses embeddings from files and converts to Embeddings structure\n",
    "    '''\n",
    "    wtoi = defaultdict(lambda: len(wtoi))\n",
    "    ctoi = defaultdict(lambda: len(ctoi))\n",
    "    [ctoi[i] for i in spec_toks]\n",
    "    [wtoi[i] for i in spec_toks]\n",
    "    \n",
    "    w_dump = open(join(emb_dir, word_emb_filename))\n",
    "    c_dump = open(join(emb_dir, char_emb_filename))\n",
    "    \n",
    "    w_emb = [np.zeros(WORD_EMB_SIZE) for i in range(len(spec_toks))]\n",
    "    c_emb = copy.deepcopy(w_emb)\n",
    "    \n",
    "    print(\"Loading word embeddings...\")\n",
    "    with open(join(emb_dir, word_emb_filename)) as w_dump:\n",
    "        for line in w_dump:\n",
    "            line = line.split(' ')\n",
    "\n",
    "            wtoi[line[0]]\n",
    "            w_emb.append(np.array([float(i) for i in line[1:]]))\n",
    "            if len(w_emb) != len(wtoi):\n",
    "                w_emb.pop()\n",
    "    \n",
    "    print(\"Loading char embeddings...\")\n",
    "    with open(join(emb_dir, char_emb_filename)) as c_dump:\n",
    "        for line in c_dump:\n",
    "            line = line.split(' ')\n",
    "\n",
    "            ctoi[line[0]]\n",
    "            c_emb.append(np.array([float(i) for i in line[1:]]))\n",
    "    \n",
    "    embed.id2wd.update({ctoi[ch]: ch for ch in ctoi})\n",
    "    embed.id2wd.update({wtoi[wd]: wd for wd in wtoi})\n",
    "    embed.wd2id = dict(wtoi)\n",
    "    embed.ch2id = dict(ctoi)\n",
    "    embed.word_emb = np.array(w_emb)\n",
    "    embed.char_emb = np.array(c_emb)\n",
    "    print(\"Embeddings are parsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T09:58:45.962627Z",
     "start_time": "2018-05-08T09:58:45.813013Z"
    }
   },
   "outputs": [],
   "source": [
    "def tockenize_squad_dataset(json_data, dataset: Dataset, emb: Embeddings):\n",
    "    tokenize = spacy.blank('en')\n",
    "\n",
    "    def tokens_pos(text, text_tok):\n",
    "        pos = 0\n",
    "        new_ind = []\n",
    "        \n",
    "        for token in text_tok:\n",
    "            pos = text.find(token, pos)\n",
    "            new_ind.append((pos, pos + len(token)))\n",
    "            pos += len(token)\n",
    "        return new_ind\n",
    "    \n",
    "    \n",
    "    cont_id, ques_id, answ_id = 0, 0, 0\n",
    "    for item in json_data:\n",
    "        for para in item['paragraphs']:\n",
    "            context = para['context'].replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
    "            contx_toks = [tk.text for tk in tokenize(context)]\n",
    "            conxt_tok2id = [emb.wd2id.get(tk, emb.wd2id['<UNK>']) for tk in contx_toks]\n",
    "            \n",
    "            if (len(contx_toks) > CONTEXT_MAX_LEN):\n",
    "                continue\n",
    "            \n",
    "            cont_tok_posit = tokens_pos(context, contx_toks)\n",
    "            \n",
    "            for qas in para['qas']:\n",
    "                ques = qas['question'].replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
    "                ques_tok2id = [emb.wd2id.get(tk.text, emb.wd2id['<UNK>']) for tk in tokenize(ques)]\n",
    "                question_id = qas['id']\n",
    "                \n",
    "                for answ in qas['answers']:\n",
    "                    answ_text = answ['text']\n",
    "                    answ_start = answ['answer_start']\n",
    "                    answ_end = answ_start + len(answ_text)\n",
    "                    answ_fit_toks = []\n",
    "                    # taking context tokens which are inside of answer borders\n",
    "                    for idx, posit in enumerate(cont_tok_posit):\n",
    "                        if answ_start < posit[1] and answ_end > posit[0]:\n",
    "                            answ_fit_toks.append(idx)\n",
    "                    answ_pair = (answ_fit_toks[0], answ_fit_toks[-1])\n",
    "                    \n",
    "                    dataset.answers.append(answ_pair)\n",
    "                    dataset.packs.append((cont_id, ques_id, answ_id))\n",
    "                    answ_id += 1\n",
    "                \n",
    "                dataset.questions.append(ques_tok2id)\n",
    "                dataset.questions_id.append(question_id)\n",
    "                ques_id += 1\n",
    "            \n",
    "            dataset.contexts.append(conxt_tok2id)\n",
    "            cont_id += 1\n",
    "        dataset.length = len(dataset.packs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-04T10:43:26.937564Z",
     "start_time": "2018-05-04T10:43:26.922362Z"
    }
   },
   "outputs": [],
   "source": [
    "# def drop_useless_embs(passages_tok, embed, new_wemb, new_wd2id):\n",
    "#     new_texts_tok = []\n",
    "#     for text in passages_tok:\n",
    "#         text_tok = []\n",
    "#         for idx, tok in enumerate(text):\n",
    "#             word = embed.id2wd[idx]\n",
    "            \n",
    "#             if word not in new_wd2id:\n",
    "#                 new_wd2id[word]\n",
    "#                 new_wemb.append(embed.word_emb[tok])\n",
    "            \n",
    "#             text_tok.append(new_wd2id[word])\n",
    "#         new_texts_tok.append(text_tok)\n",
    "#     return new_texts_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-04T19:40:46.226279Z",
     "start_time": "2018-05-04T19:40:46.211643Z"
    }
   },
   "outputs": [],
   "source": [
    "# def collect_data(dataset: Dataset, embed: Embeddings):\n",
    "#     '''\n",
    "#     Collects parsed dataset and loaded embeddings into Data class.\n",
    "#     Also embeddings are cleared from unused words.\n",
    "#     ------\n",
    "#     returns Data class.\n",
    "#     '''\n",
    "    \n",
    "#     wtoi = defaultdict(lambda : len(wtoi))\n",
    "#     [wtoi[tk] for tk in spec_toks]\n",
    "#     w_emb = [np.zeros(WORD_EMB_SIZE) for _ in spec_toks]\n",
    "#     dataset.contexts = drop_useless_embs(dataset.contexts, embed.word_emb, w_emb, wtoi)\n",
    "#     dataset.questions = drop_useless_embs(dataset.questions, embed.word_emb, w_emb, wtoi)\n",
    "#     itow = {}\n",
    "#     for word in wtoi:\n",
    "#         itow[wtoi[word]] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T10:05:40.411584Z",
     "start_time": "2018-05-08T09:58:55.796905Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word embeddings...\n",
      "Loading char embeddings...\n",
      "Embeddings are parsed.\n"
     ]
    }
   ],
   "source": [
    "embed = Embeddings()\n",
    "parse_glove_embeddings(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T10:08:14.947252Z",
     "start_time": "2018-05-08T10:05:44.424194Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "\n",
    "f = open(os.path.join(dataset_dir, train_filename), 'r')\n",
    "train = uj.load(f)['data']\n",
    "f.close()\n",
    "    \n",
    "tockenize_squad_dataset(train, dataset, embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-05T11:38:38.854845Z",
     "start_time": "2018-05-05T11:38:38.814660Z"
    }
   },
   "outputs": [],
   "source": [
    "# packs = trunk(dataset.packs, 32)\n",
    "# Cw, Cc, Qw, Qc, a = to_batch(packs[0], embed, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T10:20:30.901564Z",
     "start_time": "2018-05-08T10:20:30.863135Z"
    }
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.sigma = None\n",
    "        self.mu = None\n",
    "\n",
    "        \n",
    "    def forward(self, z):\n",
    "        if z.size(1) == 1:\n",
    "            return z\n",
    "        \n",
    "        if self.sigma is None:\n",
    "            self.sigma = nn.Parameter(torch.ones(z.shape))\n",
    "            self.mu = nn.Parameter(torch.zeros(z.shape))\n",
    "        \n",
    "        mu = torch.mean(z, keepdim=True, dim=-1)\n",
    "        sigma = torch.std(z, keepdim=True, dim=-1)\n",
    "        out = (z - mu) / (sigma + self.eps)\n",
    "        out = out * self.sigma.expand_as(out) + self.mu.expand_as(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T10:20:34.483139Z",
     "start_time": "2018-05-08T10:20:34.400381Z"
    }
   },
   "outputs": [],
   "source": [
    "class DepthwiseSeparableConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=0, bias=False):\n",
    "        super().__init__()\n",
    "        self.in_ch = in_channels\n",
    "        self.out_ch = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = 0\n",
    "        self.bias = bias\n",
    "\n",
    "        self.depthwise = nn.Conv1d(in_channels, in_channels, kernel_size, \n",
    "                                   padding=padding, bias=bias)\n",
    "        self.pointwise = nn.Conv1d(in_channels, out_channels, 1, padding=0, bias=bias)\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.pointwise(self.depthwise(input))\n",
    "    \n",
    "    \n",
    "class DepthwiseSeparableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=0, bias=False):\n",
    "        super().__init__()\n",
    "        self.in_ch = in_channels\n",
    "        self.out_ch = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = 0\n",
    "        self.bias = bias\n",
    "\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, \n",
    "                                   padding=padding, bias=bias)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, (1, 1), padding=0, bias=bias)\n",
    "        \n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.pointwise(self.depthwise(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T10:20:37.284132Z",
     "start_time": "2018-05-08T10:20:37.256826Z"
    }
   },
   "outputs": [],
   "source": [
    "class HighwayNet(nn.Module):\n",
    "    def __init__(self, n_lay, dim, bias=True):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_lay\n",
    "        self.dim = dim\n",
    "        self.with_bias = bias\n",
    "        self.W_H_lay = [nn.Linear(dim, dim, bias=self.with_bias) for _ in range(self.n_layers)]\n",
    "        self.W_T_lay = [nn.Linear(dim, dim, bias=self.with_bias) for _ in range(self.n_layers)]\n",
    "        self.non_lin = nn.ReLU()\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(self.n_layers):\n",
    "            H = self.non_lin(self.W_H_lay[i](x))\n",
    "            T = self.non_lin(self.W_T_lay[i](x))\n",
    "            x = H * T + x - x * T\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T10:30:00.968975Z",
     "start_time": "2018-05-08T10:30:00.865569Z"
    }
   },
   "outputs": [],
   "source": [
    "class WordCharEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.highway = HighwayNet(N_HIGHWAY_LAYERS, MODEL_DIM)\n",
    "        self.conv1d = DepthwiseSeparableConv1d(WORD_EMB_SIZE + CHAR_EMB_SIZE, MODEL_DIM, WORDCHAR_EMBED_KERSIZE,\n",
    "                                               padding=WORDCHAR_EMBED_KERSIZE//2, bias=True)\n",
    "        self.conv2d = DepthwiseSeparableConv2d(CHAR_EMB_SIZE, CHAR_EMB_SIZE, WORDCHAR_EMBED_KERSIZE,\n",
    "                                               padding=WORDCHAR_EMBED_KERSIZE//2, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout_wd = nn.Dropout(DROP_PROB_WD)\n",
    "        self.dropout_ch = nn.Dropout(DROP_PROB_CH)\n",
    "        \n",
    "    def forward(self, wd_emb, ch_emb):\n",
    "        '''\n",
    "        wd_emb: DoubleTensor of shape (batch_sz, context(question)_max_len, word_emb_size)\n",
    "        ch_emb: DoubleTensor of shape (batch_sz, context(question)_max_len, word_max_len, char_emb_size)\n",
    "        '''\n",
    "        \n",
    "        # Changing order of dimention to make channels dimention go first for feeding to Conv layers.\n",
    "        wd_emb = wd_emb.transpose(1, 2)\n",
    "        ch_emb = ch_emb.permute(0, 3, 1, 2)\n",
    "        \n",
    "        wd_emb = self.dropout_wd(wd_emb)\n",
    "        ch_emb = self.dropout_ch(ch_emb)\n",
    "        \n",
    "        ch_emb = self.conv2d(ch_emb)\n",
    "        ch_emb = self.relu(ch_emb)\n",
    "        \n",
    "        ch_emb, _ = torch.max(ch_emb, dim=3) \n",
    "        wd_ch_conc = torch.cat([wd_emb, ch_emb], dim=1)\n",
    "        \n",
    "        wd_ch_conv = self.conv1d(wd_ch_conc).transpose(1, 2)\n",
    "        wd_ch_highway = self.highway(wd_ch_conv)\n",
    "        return wd_ch_highway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T10:20:43.941586Z",
     "start_time": "2018-05-08T10:20:43.899516Z"
    }
   },
   "outputs": [],
   "source": [
    "class ScaledDotProduct(nn.Module):\n",
    "    def __init__(self, keys_dim, p=DROP_PROB_ATTEN):\n",
    "        super().__init__()\n",
    "        self.scale = 1 / np.sqrt(keys_dim)\n",
    "        self.dropout = nn.Dropout(p)        \n",
    "\n",
    "    \n",
    "    def forward(self, q, v, k, mask=None):\n",
    "        attention = torch.bmm(q, k.transpose(1, 2)) * self.scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            attention.data.masked_fill_(mask, -float('inf'))\n",
    "        attention = F.softmax(attention, dim=2)\n",
    "        attention = self.dropout(attention)\n",
    "        return torch.bmm(attention, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T10:20:47.269318Z",
     "start_time": "2018-05-08T10:20:47.167095Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, batch_sz, d_model, d_k, d_v, p=DROP_PROB_ATTEN):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.WQ_i = [torch.empty((batch_sz, d_model, d_k), device=device, requires_grad=True) \n",
    "                     for _ in range(n_heads)]\n",
    "        self.WK_i = [torch.empty((batch_sz, d_model, d_k), device=device, requires_grad=True) \n",
    "                     for _ in range(n_heads)]\n",
    "        self.WV_i = [torch.empty((batch_sz, d_model, d_v), device=device, requires_grad=True) \n",
    "                     for _ in range(n_heads)]\n",
    "        self.WO = torch.empty((batch_sz, self.n_heads * d_v, d_model), device=device, requires_grad=True)\n",
    "        nn.init.xavier_normal_(self.WO)\n",
    "        for i in range(n_heads):\n",
    "            nn.init.xavier_normal_(self.WQ_i[i])\n",
    "            nn.init.xavier_normal_(self.WK_i[i])\n",
    "            nn.init.xavier_normal_(self.WV_i[i])\n",
    "        \n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.scal_dot = ScaledDotProduct(d_k)\n",
    "    \n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        heads = []\n",
    "        for i in range(self.n_heads):\n",
    "            QW = torch.bmm(q, self.WQ_i[i])\n",
    "            KW = torch.bmm(k, self.WK_i[i])\n",
    "            VW = torch.bmm(v, self.WV_i[i])\n",
    "            heads.append(self.scal_dot(QW, KW, VW))\n",
    "        con = torch.cat(heads, dim=-1)\n",
    "        output = torch.bmm(con, self.WO)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T10:20:50.378094Z",
     "start_time": "2018-05-08T10:20:50.360591Z"
    }
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.d_k = dim // N_HEADS\n",
    "        self.d_v = self.d_k\n",
    "        self.multihead_att = MultiHeadAttention(N_HEADS, BATCH_SIZE, dim, \n",
    "                                                self.d_k, self.d_v)\n",
    "    \n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.multihead_att(input, input, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T10:20:53.203906Z",
     "start_time": "2018-05-08T10:20:53.044717Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):    \n",
    "    def __init__(self, num_convs, chan_num, kern_sz, mod_dim):\n",
    "        super().__init__()\n",
    "        self.drop_p = DROP_PROB_INNER\n",
    "        self.is_train = TENSORS_IS_TRAINABLE\n",
    "        self.convs = [DepthwiseSeparableConv1d(chan_num, chan_num, kern_sz, padding=kern_sz//2)\n",
    "                      for _ in range(num_convs)]\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer_norm = LayerNorm()\n",
    "        self.selfattention = SelfAttention(mod_dim)\n",
    "        self.lin = nn.Linear(chan_num, chan_num, bias=False)\n",
    "        self.freqs = np.array([1 / np.power(10000, 2 * i / (mod_dim)) \n",
    "                               for i in range(mod_dim)])\n",
    "\n",
    "\n",
    "    \n",
    "    def position_encoding(self, x):\n",
    "        (_, text_len, dim) = x.shape\n",
    "        encoding = np.array([pos * self.freqs for pos in range(text_len)])\n",
    "        enc_t = torch.empty(x.shape)\n",
    "        enc_t[:] = torch.Tensor(encoding, device=device)\n",
    "        return enc_t + x\n",
    "    \n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.position_encoding(input)\n",
    "        residual = output\n",
    "        \n",
    "        for i in range(len(self.convs)):\n",
    "            output = self.layer_norm(output).transpose(1, 2)\n",
    "            output = self.convs[i](output)\n",
    "            output = self.relu(output).transpose(1, 2)\n",
    "            output += residual\n",
    "            output = F.dropout(output, p=self.drop_p, training=self.is_train)\n",
    "            residual = output\n",
    "        \n",
    "        output = self.layer_norm(output)\n",
    "        output = self.selfattention(output)\n",
    "        output += residual\n",
    "        output = F.dropout(output, p=self.drop_p, training=self.is_train)\n",
    "        residual = output\n",
    "        \n",
    "        output = self.layer_norm(output)\n",
    "        output = self.lin(output)\n",
    "        output = self.relu(output)\n",
    "        output += residual\n",
    "        output = F.dropout(output, p=self.drop_p, training=self.is_train)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T10:20:56.116237Z",
     "start_time": "2018-05-08T10:20:55.989078Z"
    }
   },
   "outputs": [],
   "source": [
    "class ContextQueryAttention(nn.Module):\n",
    "    def __init__(self, p=DROP_PROB_INNER):\n",
    "        super().__init__()\n",
    "        self.W0 = torch.empty((BATCH_SIZE, 1, 3 * MODEL_DIM), device=device, requires_grad=True)\n",
    "        nn.init.xavier_normal_(self.W0)\n",
    "        self.dropout = nn.AlphaDropout(p)\n",
    "    \n",
    "    \n",
    "    def forward(self, context, query):\n",
    "        n = context.shape[1]\n",
    "        m = query.shape[1]\n",
    "        S = torch.empty((BATCH_SIZE, n, m), device=device)\n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                S[:, i, j] = self.similarity_function(context[:, i, :], query[:, j, :])\n",
    "                \n",
    "        S_1 = F.softmax(S, dim=2)\n",
    "        S_2 = F.softmax(S, dim=1)\n",
    "        S_mul_12 = torch.bmm(S_1, S_2.transpose(1, 2))\n",
    "        A = torch.bmm(S_1, query)\n",
    "        B = torch.bmm(S_mul_12, context)\n",
    "        output = torch.cat([context, A, context * A, context * B], dim=-1)\n",
    "        output = self.dropout(output)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def similarity_function(self, c, q):\n",
    "        con = torch.cat([q, c, q * c], dim=-1).unsqueeze(dim=2)\n",
    "        return torch.bmm(self.W0, con).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T10:23:19.291035Z",
     "start_time": "2018-05-08T10:23:19.250099Z"
    }
   },
   "outputs": [],
   "source": [
    "class AnswerStartEnd(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W0 = torch.empty(BATCH_SIZE, 1, 4 * MODEL_DIM, device=device, requires_grad=True)\n",
    "        self.W1 = torch.empty(BATCH_SIZE, 1, 4 * MODEL_DIM, device=device, requires_grad=True)\n",
    "        nn.init.xavier_normal_(self.W0)\n",
    "        nn.init.xavier_normal_(self.W1)\n",
    "    \n",
    "    \n",
    "    def forward(self, M0, M1, M2):\n",
    "        cat_M01 = torch.cat([M0, M1], dim=-1)\n",
    "        cat_M02 = torch.cat([M0, M2], dim=-1)\n",
    "        Y0 = torch.bmm(self.W0, cat_M01.transpose(1, 2)).squeeze()\n",
    "        Y1 = torch.bmm(self.W1, cat_M02.transpose(1, 2)).squeeze()\n",
    "        p1 = F.softmax(Y0, dim=1)\n",
    "        p2 = F.softmax(Y1, dim=1)\n",
    "        return p1, p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T10:30:17.700991Z",
     "start_time": "2018-05-08T10:30:17.442361Z"
    }
   },
   "outputs": [],
   "source": [
    "class QANet(nn.Module):\n",
    "    def __init__(self, embeddings: Embeddings, char_pretrained=False):\n",
    "        super().__init__()\n",
    "        self.char_emb = None\n",
    "        if char_pretrained:\n",
    "            self.char_emb = nn.Embedding.from_pretrained(torch.Tensor(embeddings.char_emb))\n",
    "        else:\n",
    "            self.char_emb = nn.Embedding(ASCII_LEN, CHAR_EMB_SIZE)\n",
    "        \n",
    "        self.word_emb = nn.Embedding.from_pretrained(torch.Tensor(embeddings.word_emb))\n",
    "        self.emb = WordCharEmbedding()\n",
    "        self.emb_enc = Encoder(N_EMBED_ENC_CONVS, MODEL_DIM, EMBED_ENC_KERSIZE, MODEL_DIM)\n",
    "        self.mod_enc = Encoder(N_MODEL_ENC_CONVS, MODEL_DIM * 2, MODEL_ENC_KERSIZE, 2 * MODEL_DIM)\n",
    "        self.con_quer_atten = ContextQueryAttention()\n",
    "        self.downscale = DepthwiseSeparableConv1d(4 * MODEL_DIM, 2 * MODEL_DIM, \n",
    "                                                  DOWNSCALE_KERSIZE, padding=DOWNSCALE_KERSIZE//2)\n",
    "        self.answ = AnswerStartEnd()\n",
    "    \n",
    "    \n",
    "    def forward(self, cont_word_idx, cont_char_idx, ques_word_idx, ques_char_idx):\n",
    "        '''\n",
    "        cont_word_idx: LongTensor (batch_sz, max_context_len)\n",
    "        cont_char_idx: LongTensor (batch_sz, max_context_len, max_word_len)\n",
    "        ques_word_idx: LongTensor (batch_sz, max_question_len)\n",
    "        ques_char_idx: LongTensor (batch_sz, max_question_len, max_word_len)\n",
    "        '''\n",
    "        # Getting word and char embeddings for context and question\n",
    "        print(\"Getting word and char embeddings for context and question\")\n",
    "        cont_word_emb = self.word_emb(cont_word_idx)\n",
    "        cont_char_emb = self.char_emb(cont_char_idx)\n",
    "        ques_word_emb = self.word_emb(ques_word_idx)\n",
    "        ques_char_emb = self.char_emb(ques_char_idx)\n",
    "        \n",
    "        # Getting concatenated word embedding with char-level embedding\n",
    "        print(\"Getting concatenated word embedding with char-level embedding\")\n",
    "        cont_emb_out = self.emb(cont_word_emb, cont_char_emb)\n",
    "        ques_emb_out = self.emb(ques_word_emb, ques_char_emb)\n",
    "        \n",
    "        # Applying Encoding block layer\n",
    "        print(\"Applying Encoding block layer\")\n",
    "        cont_enc_emb = self.emb_enc(cont_emb_out)\n",
    "        ques_enc_emb = self.emb_enc(ques_emb_out)\n",
    "        \n",
    "        # Applying Context-Query attention and reducing channels dimension\n",
    "        print(\"Applying Context-Query attention and reducing channels dimension\")\n",
    "        cont_ques_atten = self.con_quer_atten(cont_enc_emb, ques_enc_emb).transpose(1, 2)\n",
    "        print(cont_ques_atten.shape)\n",
    "        cq_atten_resized = self.downscale(cont_ques_atten).transpose(1, 2)\n",
    "        \n",
    "        # Model Encoding Blocks\n",
    "        print(\"Model Encoding Blocks:\\n M0\")\n",
    "        M0 = cq_atten_resized\n",
    "        for i in range(1):\n",
    "            M0 = self.mod_enc(M0)\n",
    "        \n",
    "        M1 = torch.Tensor(M0)\n",
    "        print(\"Model Encoding Blocks:\\n M1\")\n",
    "        for i in range(1):\n",
    "            M1 = self.mod_enc(M1)\n",
    "        \n",
    "        M2 = torch.Tensor(M1)\n",
    "        print(\"Model Encoding Blocks:\\n M2\")\n",
    "        for i in range(1):\n",
    "            M2 = self.mod_enc(M2)\n",
    "        \n",
    "        # Getting positions probabilities\n",
    "        print(\"Getting positions probabilities\")\n",
    "        p1, p2 = self.answ(M0, M1, M2)\n",
    "        return p1, p2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and batchif"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "415px",
    "left": "607px",
    "right": "240px",
    "top": "113px",
    "width": "433px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
