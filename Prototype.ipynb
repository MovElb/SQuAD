{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-05T18:11:00.879183Z",
     "start_time": "2018-05-05T18:11:00.860521Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import os\n",
    "from os.path import exists, join\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "import spacy\n",
    "\n",
    "import copy\n",
    "\n",
    "import ujson as uj\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-05T17:15:24.715042Z",
     "start_time": "2018-05-05T17:15:24.696937Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_dir = '../Data/Datasets'\n",
    "emb_dir = '../Data/Embeddings'\n",
    "\n",
    "train_filename = 'train-v1.1.json'\n",
    "dev_filename = 'dev-v1.1.json'\n",
    "char_emb_filename = \"glove.840B.300d-char.txt\"\n",
    "word_emb_zip = \"glove.840B.300d.zip\"\n",
    "word_emb_filename = \"glove.840B.300d.txt\"\n",
    "\n",
    "word_emb_url_base = \"http://nlp.stanford.edu/data/\"\n",
    "char_emb_url_base = \"https://raw.githubusercontent.com/minimaxir/char-embeddings/master/\"\n",
    "train_url_base = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/\"\n",
    "dev_url_base = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/\"\n",
    "\n",
    "spec_toks = ['<UNK>', '<PAD>']\n",
    "\n",
    "CONTEXT_MAX_LEN = 300\n",
    "QUESTION_MAX_LEN = 60\n",
    "ASCII_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-05T17:44:14.366091Z",
     "start_time": "2018-05-05T17:44:14.361320Z"
    }
   },
   "outputs": [],
   "source": [
    "WORD_EMB_SIZE = 300\n",
    "CHAR_EMB_SIZE = 200\n",
    "N_HIGHWAY_LAYERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T13:17:18.341614Z",
     "start_time": "2018-05-03T13:17:18.331151Z"
    }
   },
   "outputs": [],
   "source": [
    "class Embeddings:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        word_emb: word embeddings\n",
    "        char_emb: char embeddings\n",
    "        wd2id: word to index\n",
    "        id2wd: index to word\n",
    "        ch2id: char to index\n",
    "        id2ch: index to char\n",
    "        '''\n",
    "        self.word_emb = np.empty(1)\n",
    "        self.char_emb = np.empty(1)\n",
    "        self.wd2id = {}\n",
    "        self.id2wd = {}\n",
    "        self.ch2id = {}\n",
    "        self.id2ch = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-04T12:29:46.994675Z",
     "start_time": "2018-05-04T12:29:46.985659Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    '''\n",
    "    length: the dataset lenght\n",
    "    context: list of passages\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.length = 0\n",
    "        self.contexts = []\n",
    "        self.questions = []\n",
    "        self.questions_id = []\n",
    "        self.answers = []\n",
    "        self.packs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T21:22:53.581184Z",
     "start_time": "2018-05-03T21:22:53.573792Z"
    }
   },
   "outputs": [],
   "source": [
    "def download(url, path, filename):\n",
    "    '''\n",
    "    Downloads the file from url/filename and saves in path/filename\n",
    "    '''\n",
    "    if (exists(join(path, filename))):\n",
    "        print(\"File {} already exists.\".format(filename))\n",
    "        return\n",
    "    print(\"Downloading file {}...\".format(filename))\n",
    "    urllib.request.urlretrieve(join(url, filename), filename=join(path, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T13:17:20.749928Z",
     "start_time": "2018-05-03T13:17:20.725558Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_all_data():\n",
    "    '''\n",
    "    Downloads embeddings and datasets\n",
    "    '''\n",
    "    dirs = [dataset_dir, emb_dir]\n",
    "    \n",
    "    for d in dirs:\n",
    "        if not exists(d):\n",
    "            os.makedirs(d)\n",
    "    \n",
    "    download(train_url_base, dataset_dir, train_filename)\n",
    "    download(dev_url_base, dataset_dir, dev_filename)\n",
    "    download(char_emb_url_base, emb_dir, char_emb_filename)\n",
    "    \n",
    "    if not os.path.exists(join(emb_dir, word_emb_filename)):\n",
    "        download(word_emb_url_base, emb_dir, word_emb_zip)\n",
    "        print(\"Unzipping file {}\".format(word_emb_zip))\n",
    "        zip_path = join(emb_dir, word_emb_zip)\n",
    "        txt_path = join(emb_dir, word_emb_filename)\n",
    "        zp = zipfile.ZipFile(zip_path)\n",
    "        zp.extractall(path=txt_path)\n",
    "        zp.close()\n",
    "        os.remove(zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T13:59:52.974646Z",
     "start_time": "2018-05-03T13:59:52.881280Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_glove_embeddings(embed: Embeddings):\n",
    "    '''\n",
    "    Parses embeddings from files and converts to Embeddings structure\n",
    "    '''\n",
    "    wtoi = defaultdict(lambda: len(wtoi))\n",
    "    ctoi = defaultdict(lambda: len(ctoi))\n",
    "    [ctoi[i] for i in spec_toks]\n",
    "    [wtoi[i] for i in spec_toks]\n",
    "    \n",
    "    w_dump = open(join(emb_dir, word_emb_filename))\n",
    "    c_dump = open(join(emb_dir, char_emb_filename))\n",
    "    \n",
    "    w_emb = [np.zeros(WORD_EMB_SIZE) for i in range(len(spec_toks))]\n",
    "    c_emb = copy.deepcopy(w_emb)\n",
    "    \n",
    "    print(\"Loading word embeddings...\")\n",
    "    with open(join(emb_dir, word_emb_filename)) as w_dump:\n",
    "        for line in w_dump:\n",
    "            line = line.split(' ')\n",
    "\n",
    "            wtoi[line[0]]\n",
    "            w_emb.append(np.array([float(i) for i in line[1:]]))\n",
    "            if len(w_emb) != len(wtoi):\n",
    "                w_emb.pop()\n",
    "    \n",
    "    print(\"Loading char embeddings...\")\n",
    "    with open(join(emb_dir, char_emb_filename)) as c_dump:\n",
    "        for line in c_dump:\n",
    "            line = line.split(' ')\n",
    "\n",
    "            ctoi[line[0]]\n",
    "            c_emb.append(np.array([float(i) for i in line[1:]]))\n",
    "    \n",
    "    embed.id2wd.update({ctoi[ch]: ch for ch in ctoi})\n",
    "    embed.id2wd.update({wtoi[wd]: wd for wd in wtoi})\n",
    "    embed.wd2id = dict(wtoi)\n",
    "    embed.ch2id = dict(ctoi)\n",
    "    embed.word_emb = np.array(w_emb)\n",
    "    embed.char_emb = np.array(c_emb)\n",
    "    print(\"Embeddings are parsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-05T11:32:35.557620Z",
     "start_time": "2018-05-05T11:32:35.417082Z"
    }
   },
   "outputs": [],
   "source": [
    "def tockenize_squad_dataset(json_data, dataset: Dataset, emb: Embeddings):\n",
    "    tokenize = spacy.blank('en')\n",
    "\n",
    "    def tokens_pos(text, text_tok):\n",
    "        pos = 0\n",
    "        new_ind = []\n",
    "        \n",
    "        for token in text_tok:\n",
    "            pos = text.find(token, pos)\n",
    "            new_ind.append((pos, pos + len(token)))\n",
    "            pos += len(token)\n",
    "        return new_ind\n",
    "    \n",
    "    \n",
    "    cont_id, ques_id, answ_id = 0, 0, 0\n",
    "    for item in json_data:\n",
    "        for para in item['paragraphs']:\n",
    "            context = para['context'].replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
    "            contx_toks = [tk.text for tk in tokenize(context)]\n",
    "            conxt_tok2id = [emb.wd2id.get(tk, emb.wd2id['<UNK>']) for tk in contx_toks]\n",
    "            \n",
    "            if (len(contx_toks) > CONTEXT_MAX_LEN):\n",
    "                continue\n",
    "            \n",
    "            cont_tok_posit = tokens_pos(context, contx_toks)\n",
    "            \n",
    "            for qas in para['qas']:\n",
    "                ques = qas['question'].replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
    "                ques_tok2id = [emb.wd2id.get(tk.text, emb.wd2id['<UNK>']) for tk in tokenize(ques)]\n",
    "                question_id = qas['id']\n",
    "                \n",
    "                for answ in qas['answers']:\n",
    "                    answ_text = answ['text']\n",
    "                    answ_start = answ['answer_start']\n",
    "                    answ_end = answ_start + len(answ_text)\n",
    "                    answ_fit_toks = []\n",
    "                    # taking context tokens which are inside of answer borders\n",
    "                    for idx, posit in enumerate(cont_tok_posit):\n",
    "                        if answ_start < posit[1] and answ_end > posit[0]:\n",
    "                            answ_fit_toks.append(idx)\n",
    "                    answ_pair = (answ_fit_toks[0], answ_fit_toks[-1])\n",
    "                    \n",
    "                    dataset.answers.append(answ_pair)\n",
    "                    dataset.packs.append((cont_id, ques_id, answ_id))\n",
    "                    answ_id += 1\n",
    "                \n",
    "                dataset.questions.append(ques_tok2id)\n",
    "                dataset.questions_id.append(question_id)\n",
    "                ques_id += 1\n",
    "            \n",
    "            dataset.contexts.append(conxt_tok2id)\n",
    "            cont_id += 1\n",
    "        dataset.length = len(dataset.packs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-04T10:43:26.937564Z",
     "start_time": "2018-05-04T10:43:26.922362Z"
    }
   },
   "outputs": [],
   "source": [
    "# def drop_useless_embs(passages_tok, embed, new_wemb, new_wd2id):\n",
    "#     new_texts_tok = []\n",
    "#     for text in passages_tok:\n",
    "#         text_tok = []\n",
    "#         for idx, tok in enumerate(text):\n",
    "#             word = embed.id2wd[idx]\n",
    "            \n",
    "#             if word not in new_wd2id:\n",
    "#                 new_wd2id[word]\n",
    "#                 new_wemb.append(embed.word_emb[tok])\n",
    "            \n",
    "#             text_tok.append(new_wd2id[word])\n",
    "#         new_texts_tok.append(text_tok)\n",
    "#     return new_texts_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-04T19:40:46.226279Z",
     "start_time": "2018-05-04T19:40:46.211643Z"
    }
   },
   "outputs": [],
   "source": [
    "# def collect_data(dataset: Dataset, embed: Embeddings):\n",
    "#     '''\n",
    "#     Collects parsed dataset and loaded embeddings into Data class.\n",
    "#     Also embeddings are cleared from unused words.\n",
    "#     ------\n",
    "#     returns Data class.\n",
    "#     '''\n",
    "    \n",
    "#     wtoi = defaultdict(lambda : len(wtoi))\n",
    "#     [wtoi[tk] for tk in spec_toks]\n",
    "#     w_emb = [np.zeros(WORD_EMB_SIZE) for _ in spec_toks]\n",
    "#     dataset.contexts = drop_useless_embs(dataset.contexts, embed.word_emb, w_emb, wtoi)\n",
    "#     dataset.questions = drop_useless_embs(dataset.questions, embed.word_emb, w_emb, wtoi)\n",
    "#     itow = {}\n",
    "#     for word in wtoi:\n",
    "#         itow[wtoi[word]] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-05T11:15:19.124324Z",
     "start_time": "2018-05-05T11:09:00.148253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word embeddings...\n",
      "Loading char embeddings...\n",
      "Embeddings are parsed.\n"
     ]
    }
   ],
   "source": [
    "embed = Embeddings()\n",
    "parse_glove_embeddings(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-05T11:34:49.500338Z",
     "start_time": "2018-05-05T11:32:37.824709Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "\n",
    "f = open(os.path.join(dataset_dir, train_filename), 'r')\n",
    "train = uj.load(f)['data']\n",
    "f.close()\n",
    "    \n",
    "tockenize_squad_dataset(train, dataset, embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-05T11:38:38.854845Z",
     "start_time": "2018-05-05T11:38:38.814660Z"
    }
   },
   "outputs": [],
   "source": [
    "# packs = trunk(dataset.packs, 32)\n",
    "# Cw, Cc, Qw, Qc, a = to_batch(packs[0], embed, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architechture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-05T18:25:25.919601Z",
     "start_time": "2018-05-05T18:25:25.896495Z"
    }
   },
   "outputs": [],
   "source": [
    "class HighwayNet(nn.Module):\n",
    "    def __init__(self, n_lay, dim, bias=True):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_lay\n",
    "        self.dim = dim\n",
    "        self.with_bias = bias\n",
    "        self.W_H_lay = [nn.Linear(dim, dim, bias=self.with_bias) for _ in range(self.n_layers)]\n",
    "        self.W_T_lay = [nn.Linear(dim, dim, bias=self.with_bias) for _ in range(self.n_layers)]\n",
    "        self.non_lin = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(self.n_layers):\n",
    "            H = self.non_lin(self.W_H_lay[i](x))\n",
    "            T = self.non_lin(self.W_T_lay[i](x))\n",
    "            x = H * T + x - x * T\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-05T22:56:11.731012Z",
     "start_time": "2018-05-05T22:56:11.644978Z"
    }
   },
   "outputs": [],
   "source": [
    "class DepthwiseSeparableConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=0, bias=False):\n",
    "        super().__init__()\n",
    "        self.in_ch = in_channels\n",
    "        self.out_ch = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = 0\n",
    "        self.bias = bias\n",
    "\n",
    "        self.depthwise = nn.Conv1d(in_channels, in_channels, kernel_size, \n",
    "                                   padding=padding, bias=bias)\n",
    "        self.pointwise = nn.Conv1d(in_channels, out_channels, 1, padding=0, bias=bias)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.pointwise(self.depthwise(input))\n",
    "    \n",
    "    \n",
    "class DepthwiseSeparableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=0, bias=False):\n",
    "        super().__init__()\n",
    "        self.in_ch = in_channels\n",
    "        self.out_ch = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = 0\n",
    "        self.bias = bias\n",
    "\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, \n",
    "                                   padding=padding, bias=bias)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, (1, 1), padding=0, bias=bias)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.pointwise(self.depthwise(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-05T21:50:39.694143Z",
     "start_time": "2018-05-05T21:50:39.684286Z"
    }
   },
   "outputs": [],
   "source": [
    "class WordCharEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.highway = Highway(N_HIGHWAY_LAYERS)\n",
    "        \n",
    "    \n",
    "    def forward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-05T14:49:14.899885Z",
     "start_time": "2018-05-05T14:49:14.892616Z"
    }
   },
   "outputs": [],
   "source": [
    "class EmbeddingEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def forward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-05T14:49:49.271898Z",
     "start_time": "2018-05-05T14:49:49.260020Z"
    }
   },
   "outputs": [],
   "source": [
    "class ContextQueryAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def forward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-05T14:50:13.424604Z",
     "start_time": "2018-05-05T14:50:13.414758Z"
    }
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def forward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-05T17:32:52.028332Z",
     "start_time": "2018-05-05T17:32:51.962456Z"
    }
   },
   "outputs": [],
   "source": [
    "class QANet(nn.Module):\n",
    "    def __init__(self, embeddings: Embeddings):\n",
    "        super().__init__()\n",
    "        self.char_emb = nn.Embedding(ASCII_LEN, CHAR_EMB_SIZE)\n",
    "        self.word_emb = nn.Embedding.from_pretrained(embeddings.word_emb)\n",
    "        self.emb = WordCharEmbedding()\n",
    "        self.emb_enc = EmbeddingEncoder()\n",
    "        self.con_quer_atten = ContextQueryAttention()\n",
    "    \n",
    "    def forward(self, cont_word_idx, cont_char_idx, ques_word_idx, ques_char_idx):\n",
    "        '''\n",
    "        cont_word_idx: LongTensor (batch_sz, max_context_len)\n",
    "        cont_char_idx: LongTensor (batch_sz, max_context_len, max_word_len)\n",
    "        ques_word_idx: LongTensor (batch_sz, max_question_len)\n",
    "        ques_char_idx: LongTensor (batch_sz, max_question_len, max_word_len)\n",
    "        '''\n",
    "        # Getting word and char embeddings for context and question\n",
    "        cont_word_emb = self.word_emb(cont_word_idx)\n",
    "        cont_char_emb = self.char_emb(cont_char_idx)\n",
    "        ques_word_emb = self.word_emb(ques_word_idx)\n",
    "        ques_char_emb = self.char_emb(ques_char_idx)\n",
    "        \n",
    "        # Getting concatenated word embedding with char-level embedding\n",
    "        cont_emb_out = self.emb(cont_word_emb, cont_char_emb)\n",
    "        ques_emb_out = self.emb(ques_word_emb, ques_char_emb)\n",
    "        \n",
    "        # Applying Encoding block layer\n",
    "        cont_enc_emb = self.emb_enc(cont_emb_out)\n",
    "        ques_enc_emb = self.emb_enc(ques_emb_out)\n",
    "        \n",
    "        # Applying Context-Query attention\n",
    "        cont_ques_atten = self.ContextQueryAttention(cont_enc_emb, ques_enc_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "415px",
    "left": "607px",
    "right": "240px",
    "top": "113px",
    "width": "433px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
