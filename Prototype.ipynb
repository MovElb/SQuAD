{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-04T22:13:43.432305Z",
     "start_time": "2018-05-04T22:13:43.415367Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import os\n",
    "from os.path import exists, join\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "import spacy\n",
    "\n",
    "import copy\n",
    "\n",
    "import ujson as uj\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-04T22:13:44.705276Z",
     "start_time": "2018-05-04T22:13:44.692928Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_dir = '../Data/Datasets'\n",
    "emb_dir = '../Data/Embeddings'\n",
    "\n",
    "train_filename = 'train-v1.1.json'\n",
    "dev_filename = 'dev-v1.1.json'\n",
    "char_emb_filename = \"glove.840B.300d-char.txt\"\n",
    "word_emb_zip = \"glove.840B.300d.zip\"\n",
    "word_emb_filename = \"glove.840B.300d.txt\"\n",
    "\n",
    "word_emb_url_base = \"http://nlp.stanford.edu/data/\"\n",
    "char_emb_url_base = \"https://raw.githubusercontent.com/minimaxir/char-embeddings/master/\"\n",
    "train_url_base = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/\"\n",
    "dev_url_base = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/\"\n",
    "\n",
    "spec_toks = ['<PAD>', '<UNK>', '<SOS>', '<EOS>']\n",
    "\n",
    "CONTEXT_MAX_LEN = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-04T22:13:45.467851Z",
     "start_time": "2018-05-04T22:13:45.464490Z"
    }
   },
   "outputs": [],
   "source": [
    "WORD_EMB_SIZE = 300\n",
    "CHAR_EMB_SIZE = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T13:17:18.341614Z",
     "start_time": "2018-05-03T13:17:18.331151Z"
    }
   },
   "outputs": [],
   "source": [
    "class Embeddings:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        word_emb: word embeddings\n",
    "        char_emb: char embeddings\n",
    "        wd2id: word to index\n",
    "        id2wd: index to word\n",
    "        ch2id: char to index\n",
    "        id2ch: index to char\n",
    "        '''\n",
    "        self.word_emb = np.empty(1)\n",
    "        self.char_emb = np.empty(1)\n",
    "        self.wd2id = {}\n",
    "        self.id2wd = {}\n",
    "        self.ch2id = {}\n",
    "        self.id2ch = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-04T12:29:46.994675Z",
     "start_time": "2018-05-04T12:29:46.985659Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    '''\n",
    "    length: the dataset lenght\n",
    "    context: list of passages\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.length = 0\n",
    "        self.contexts = []\n",
    "        self.questions = []\n",
    "        self.questions_id = []\n",
    "        self.answers = []\n",
    "        self.packs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-04T11:43:00.064763Z",
     "start_time": "2018-05-04T11:43:00.058835Z"
    }
   },
   "outputs": [],
   "source": [
    "class Data:\n",
    "    '''\n",
    "    Dataset + related Embeddings structure\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.train = Dataset()\n",
    "        self.test = Dataset()\n",
    "        self.embeds = Embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T21:22:53.581184Z",
     "start_time": "2018-05-03T21:22:53.573792Z"
    }
   },
   "outputs": [],
   "source": [
    "def download(url, path, filename):\n",
    "    '''\n",
    "    Downloads the file from url/filename and saves in path/filename\n",
    "    '''\n",
    "    if (exists(join(path, filename))):\n",
    "        print(\"File {} already exists.\".format(filename))\n",
    "        return\n",
    "    print(\"Downloading file {}...\".format(filename))\n",
    "    urllib.request.urlretrieve(join(url, filename), filename=join(path, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T13:17:20.749928Z",
     "start_time": "2018-05-03T13:17:20.725558Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_all_data():\n",
    "    '''\n",
    "    Downloads embeddings and datasets\n",
    "    '''\n",
    "    dirs = [dataset_dir, emb_dir]\n",
    "    \n",
    "    for d in dirs:\n",
    "        if not exists(d):\n",
    "            os.makedirs(d)\n",
    "    \n",
    "    download(train_url_base, dataset_dir, train_filename)\n",
    "    download(dev_url_base, dataset_dir, dev_filename)\n",
    "    download(char_emb_url_base, emb_dir, char_emb_filename)\n",
    "    \n",
    "    if not os.path.exists(join(emb_dir, word_emb_filename)):\n",
    "        download(word_emb_url_base, emb_dir, word_emb_zip)\n",
    "        print(\"Unzipping file {}\".format(word_emb_zip))\n",
    "        zip_path = join(emb_dir, word_emb_zip)\n",
    "        txt_path = join(emb_dir, word_emb_filename)\n",
    "        zp = zipfile.ZipFile(zip_path)\n",
    "        zp.extractall(path=txt_path)\n",
    "        zp.close()\n",
    "        os.remove(zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T13:59:52.974646Z",
     "start_time": "2018-05-03T13:59:52.881280Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_glove_embeddings(embed: Embeddings):\n",
    "    '''\n",
    "    Parses embeddings from files and converts to Embeddings structure\n",
    "    '''\n",
    "    wtoi = defaultdict(lambda: len(wtoi))\n",
    "    ctoi = defaultdict(lambda: len(ctoi))\n",
    "    [ctoi[i] for i in spec_toks]\n",
    "    [wtoi[i] for i in spec_toks]\n",
    "    \n",
    "    w_dump = open(join(emb_dir, word_emb_filename))\n",
    "    c_dump = open(join(emb_dir, char_emb_filename))\n",
    "    \n",
    "    w_emb = [np.zeros(WORD_EMB_SIZE) for i in range(len(spec_toks))]\n",
    "    c_emb = copy.deepcopy(w_emb)\n",
    "    \n",
    "    print(\"Loading word embeddings...\")\n",
    "    with open(join(emb_dir, word_emb_filename)) as w_dump:\n",
    "        for line in w_dump:\n",
    "            line = line.split(' ')\n",
    "\n",
    "            wtoi[line[0]]\n",
    "            w_emb.append(np.array([float(i) for i in line[1:]]))\n",
    "            if len(w_emb) != len(wtoi):\n",
    "                w_emb.pop()\n",
    "    \n",
    "    print(\"Loading char embeddings...\")\n",
    "    with open(join(emb_dir, char_emb_filename)) as c_dump:\n",
    "        for line in c_dump:\n",
    "            line = line.split(' ')\n",
    "\n",
    "            ctoi[line[0]]\n",
    "            c_emb.append(np.array([float(i) for i in line[1:]]))\n",
    "    \n",
    "    embed.id2wd.update({ctoi[ch]: ch for ch in ctoi})\n",
    "    embed.id2wd.update({wtoi[wd]: wd for wd in wtoi})\n",
    "    embed.wd2id = dict(wtoi)\n",
    "    embed.ch2id = dict(ctoi)\n",
    "    embed.word_emb = np.array(w_emb)\n",
    "    embed.char_emb = np.array(c_emb)\n",
    "    print(\"Embeddings are parsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T22:20:08.955393Z",
     "start_time": "2018-05-03T22:20:08.813537Z"
    }
   },
   "outputs": [],
   "source": [
    "def tockenize_squad_dataset(json_data, dataset: Dataset, emb: Embeddings):\n",
    "    tokenizer = spacy.blank('en')\n",
    "\n",
    "    def tokens_pos(text, text_tok):\n",
    "        pos = 0\n",
    "        new_ind = []\n",
    "        \n",
    "        for token in text_tok:\n",
    "            pos = text.find(token, pos)\n",
    "            new_ind.append((pos, pos + len(token)))\n",
    "            pos += token\n",
    "        return new_ind\n",
    "    \n",
    "    \n",
    "    cont_id, ques_id, answ_id = 0, 0, 0\n",
    "    for item in json_data:\n",
    "        for para in item['paragraphs']:\n",
    "            context = para['context'].replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
    "            contx_toks = [tk.text for tk in tokenize(context)]\n",
    "            conxt_tok2id = [emb.wd2id.getvalue(tk, wd2id['<UNK>']) for tk in cont_toks]\n",
    "            \n",
    "            if (len(contx_tok) > CONTEXT_MAX_LEN):\n",
    "                continue\n",
    "            \n",
    "            cont_tok_posit = reindex(context, contx_tok)\n",
    "            \n",
    "            for qas in para['qas']:\n",
    "                ques = qas['question'].replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
    "                ques_tok2id = [emb.wd2id.getvalue(tk.text, wd2id['<UNK>']) for tk in tokenize(ques)]\n",
    "                ques_id = qas['id']\n",
    "                \n",
    "                for answ in qas['answ']:\n",
    "                    answ_text = answ['text']\n",
    "                    answ_start = answ['start']\n",
    "                    answ_end = answ_start + len(answ_text)\n",
    "                    answ_fit_toks = []\n",
    "                    1\n",
    "                    # taking context tokens which are inside of answer borders\n",
    "                    for idx, posit in enumerate(cont_toks):\n",
    "                        if answ_start < posit[1] and answ_end > posit[0]:\n",
    "                            answ_fit_toks.append(idx)\n",
    "                    ans_pair = (ans_span[0], ans_span[-1])\n",
    "                    \n",
    "                    dataset.answers.append(ans_pair)\n",
    "                    dataset.packs.append((cid, qid, aid))\n",
    "                    answ_id += 1\n",
    "                \n",
    "                dataset.questions.append(ques_tok2id)\n",
    "                dataset.question_ids.append(ques_id)\n",
    "                ques_id += 1\n",
    "            \n",
    "            dataset.contexts.append(conxt_tok2id)\n",
    "            cont_id += 1\n",
    "        dataset.length = len(dataset.packs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-04T10:43:26.937564Z",
     "start_time": "2018-05-04T10:43:26.922362Z"
    }
   },
   "outputs": [],
   "source": [
    "def drop_useless_embs(passages_tok, embed, new_wemb, new_wd2id):\n",
    "    new_texts_tok = []\n",
    "    for text in passages_tok:\n",
    "        text_tok = []\n",
    "        for idx, tok in enumerate(text):\n",
    "            word = embed.id2wd[idx]\n",
    "            \n",
    "            if word not in new_wd2id:\n",
    "                new_wd2id[word]\n",
    "                new_wemb.append(embed.word_emb[tok])\n",
    "            \n",
    "            text_tok.append(new_wd2id[word])\n",
    "        new_texts_tok.append(text_tok)\n",
    "    return new_texts_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-04T19:40:46.226279Z",
     "start_time": "2018-05-04T19:40:46.211643Z"
    }
   },
   "outputs": [],
   "source": [
    "def collect_data(dataset: Dataset, embed: Embeddings):\n",
    "    '''\n",
    "    Collects parsed dataset and loaded embeddings into Data class.\n",
    "    Also embeddings are cleared from unused words.\n",
    "    ------\n",
    "    returns Data class.\n",
    "    '''\n",
    "    \n",
    "    wtoi = defaultdict(lambda : len(wtoi))\n",
    "    [wtoi[tk] for tk in spec_toks]\n",
    "    w_emb = [np.zeros(WORD_EMB_SIZE) for _ in spec_toks]\n",
    "    dataset.contexts = drop_useless_embs(dataset.contexts, embed.word_emb, w_emb, wtoi)\n",
    "    dataset.questions = drop_useless_embs(dataset.questions, embed.word_emb, w_emb, wtoi)\n",
    "    itow = {}\n",
    "    for word in wtoi:\n",
    "        itow[wtoi[word]] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-04T22:16:58.948270Z",
     "start_time": "2018-05-04T22:16:58.945081Z"
    }
   },
   "outputs": [],
   "source": [
    "t = torch.Tensor(2, 1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-04T22:17:07.998514Z",
     "start_time": "2018-05-04T22:17:07.993977Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.view(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "415px",
    "left": "607px",
    "right": "240px",
    "top": "113px",
    "width": "433px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
